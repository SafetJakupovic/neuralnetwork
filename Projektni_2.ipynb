{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"PfGhIs3CkLbc"},"source":["Mislio sam koristiti PyTorch jer sigurno neću ručno sve pisati ako se budem ikad ovim bavio, ali sam odlučio iskoristiti \"biblioteku\" sa laboratorijskih da se bolje upoznam sa stvanom funkcionalnošću takvih metoda."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":424,"status":"ok","timestamp":1686651445062,"user":{"displayName":"Filip Topic","userId":"18043200965777659690"},"user_tz":-120},"id":"UUpTrTGS7AU4"},"outputs":[],"source":["import numpy as np\n","from sklearn.datasets import fetch_covtype\n","from sklearn.model_selection import train_test_split, ParameterGrid\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_score, recall_score, accuracy_score"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1686651445521,"user":{"displayName":"Filip Topic","userId":"18043200965777659690"},"user_tz":-120},"id":"s27ySV8W7Qnq"},"outputs":[],"source":["class NeuralNetwork:\n","    def __init__(self, layers):\n","        self.layers = layers\n","\n","    # sekvencijalna propagacija x za svaki sloj \n","    # rezultat svakog postaje ulaz za sljedeci sloj\n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer.forward(x)\n","        return x\n","\n","    # uzima gradijent grad i sekvencijalno primjenjuje operaciju propagacije unazad za svaki sloj (koji su u obrnutom redoslijedu)\n","    # gradijent se azurira i prosljedjuje prethodnom sloju kao ulaz\n","    def backward(self, grad):\n","        for layer in reversed(self.layers):\n","            grad = layer.backward(grad)\n","        return grad\n","\n","    # vraca listu svih parametara \n","    def parameters(self):\n","        params = []\n","        for layer in self.layers:\n","            if hasattr(layer, 'parameters') and callable(getattr(layer, 'parameters')):\n","                params.extend(layer.parameters())\n","        return params\n","\n","    # resetuje gradijente svih slojeva\n","    def zero_grad(self):\n","        for layer in self.layers:\n","            if hasattr(layer, 'zero_grad') and callable(getattr(layer, 'zero_grad')):\n","                layer.zero_grad()"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1686651445523,"user":{"displayName":"Filip Topic","userId":"18043200965777659690"},"user_tz":-120},"id":"wvKmM35aP6DN"},"outputs":[],"source":["class Linear:\n","    # nasumicno inicijalizuje tezinsku matricu u odredjenom opsegu, kao i matricu pomjeraja u manjem opsegu\n","    # inicijalizuje gradijente kao nulte nizove istog oblika kao respektivne matrice\n","    def __init__(self, in_features, out_features):\n","        limit = np.sqrt(6 / (in_features + out_features))\n","        self.W = np.random.uniform(-limit, limit, (out_features, in_features))\n","        limit /= 100\n","        self.b = np.random.uniform(-limit, limit, out_features)\n","\n","        self.grad_w = np.zeros_like(self.W)\n","        self.grad_b = np.zeros_like(self.b)\n","    \n","    # prolaz kroz linearni sloj\n","    # mnozi numpy niz x sa transponovanom tezinskom matricom i dodaje pomjeraj b sto se vraca kao izlaz sloja \n","    def forward(self, x: np.ndarray):\n","        self.x = x\n","        return np.matmul(x, self.W.T) + self.b\n","\n","    # propagacija unazad kroz sloj\n","    # racuna gradijente sa parametrima sloja i azurira ih sumirajuci ih\n","    # vraca gradijent mnozeci proslijedjeni (grad) sa tezinskom matricom\n","    def backward(self, grad: np.ndarray):\n","        self.grad_w += np.matmul(grad.T, self.x)\n","        self.grad_b += np.sum(grad, axis=0)\n","        return np.matmul(grad, self.W)\n","\n","    # vraca parametre, koristiti se u NeuralNetwork klasi\n","    def parameters(self):\n","        return [[self.W, self.grad_w], [self.b, self.grad_b]]\n","\n","    # resetuje gradijente sloja\n","    def zero_grad(self):\n","        self.grad_w.fill(0)\n","        self.grad_b.fill(0)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1686651445524,"user":{"displayName":"Filip Topic","userId":"18043200965777659690"},"user_tz":-120},"id":"si-HnxI0UcN5"},"outputs":[],"source":["class Sigmoid:\n","    # prolaz sigmoid aktivacione funkcije\n","    # uzima numpy niz i primjenjuje sigmoid funkciju\n","    # to cuva u self.y, ali je i vraca\n","    def forward(self, x):\n","        self.y = np.divide(1, 1 + np.exp(-x))\n","        return self.y\n","\n","    # propagacija unalaz sigmoid aktivacione funkcije\n","    # uzima numpy niz grad i vraca ga pomnozen sa izvodom sigmoid funkcije\n","    def backward(self, grad):\n","        return grad * self.y * (1 - self.y)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1686651445526,"user":{"displayName":"Filip Topic","userId":"18043200965777659690"},"user_tz":-120},"id":"jP6sS-qTS3F0"},"outputs":[],"source":["# normalizuje izlaz tako da predstavljaju vjerovatnoce cija suma mora biti 1\n","class Softmax:\n","    # prolaz kroz softmax aktivacionu funkciju \n","    # primjenjuje softmax funkciju na ulaz da bi izlaz predstavljao vjerovatnoce\n","    def forward(self, x):\n","        # racuna se maksimalna vrijednost kroz zadnju osu x-a i oduzima se od od x (opet numpy niz) da se osigura numericka stabilnost onemogucujuci velike eksponencijalne vrijednosti\n","        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n","        # rezultat se vraca kao izlaz aktivacione funkcije\n","        return e_x / np.sum(e_x, axis=-1, keepdims=True)\n","\n","    # propagacija unazad kroz softmax\n","    # posto softmax ne uvodi nikakvu nelinearnost gradijent ostaje nepromijenjen\n","    def backward(self, grad):\n","        return grad"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1686651445527,"user":{"displayName":"Filip Topic","userId":"18043200965777659690"},"user_tz":-120},"id":"5LmpJMDbU3hc"},"outputs":[],"source":["class CrossEntropyLoss:\n","    def forward(self, y_pred, y):\n","        self.y_pred = y_pred\n","        self.y = y\n","        # + 1e-7 da se ne racuna logaritam nule\n","        return -np.sum(y * np.log(y_pred + 1e-7)) / len(y)\n","\n","    # racuna gradijent greske\n","    def backward(self):\n","        return (self.y_pred - self.y) / len(self.y)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1686651445528,"user":{"displayName":"Filip Topic","userId":"18043200965777659690"},"user_tz":-120},"id":"zg1JSlv0VLnV"},"outputs":[],"source":["class SGD:\n","    def __init__(self, parameters, lr):\n","        # lista nizova parametara sa gradijentima\n","        self.parameters = parameters\n","        # learning rate\n","        self.lr = lr\n","\n","    # azuriranje parametara\n","    def step(self):\n","        for param, grad in self.parameters:\n","            param -= self.lr * grad"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":472,"status":"ok","timestamp":1686652155029,"user":{"displayName":"Filip Topic","userId":"18043200965777659690"},"user_tz":-120},"id":"C_UVXca5Wxkn"},"outputs":[],"source":["def evaluate_model(model, x, y):\n","    y_pred = model.forward(x)\n","    y_pred_labels = np.argmax(y_pred, axis=1) + 1\n","    y_true_labels = np.argmax(y, axis=1) + 1\n","    precision = precision_score(y_true_labels, y_pred_labels, average='weighted', zero_division=0)\n","    recall = recall_score(y_true_labels, y_pred_labels, average='weighted', zero_division=0)\n","    # print(f\"Preciznost: {precision:.5f}\")\n","    # print(f\"Odziv: {recall:.5f}\")\n","    return precision, recall"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":2349,"status":"ok","timestamp":1686657269219,"user":{"displayName":"Filip Topic","userId":"18043200965777659690"},"user_tz":-120},"id":"7GvbwatvVYMP"},"outputs":[],"source":["# hiperparametri\n","epochs = 5\n","#learning_rate = 0.35\n","#batch_size = 12\n","\n","# input/output velicine\n","in_features = 54    # 54 je dimenzija podataka, tako i na sajtu pise, pa cu tako koristiti za unos \n","out_features = 7    # 7 klasa\n","\n","# koliko se epoha tolerise (2. dio zadatka)\n","tolerancy = 2\n","\n","# prostor hiperparametara (3. dio zadatka)\n","parameter_grid = {\n","    'learning_rate': [0.35, 0.035],\n","    'hidden_units': [(100, 50), (200, 100)],\n","    'batch_size': [12, 24]\n","}\n","# sve kombinacije parametara\n","parameter_combinations = list(ParameterGrid(parameter_grid))  # htio sam staviti po 3 za svaku, ali to je 27 kombinacija pa mi je predugo za isprobavanje\n","best_precision = 0.0\n","best_parameters = None\n","\n","# ucitavanje, preprocesiranje, oblikovanje i normalizacija unosa\n","covtype_dataset = fetch_covtype()\n","\n","x_train, y_train = covtype_dataset.data, covtype_dataset.target\n","x_train, y_train = np.array(x_train), np.array(y_train)\n","x_train = (x_train - np.mean(x_train)) / np.std(x_train)\n","y_train = np.eye(out_features)[y_train - 1]\n","\n","x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42) # 20% je valjda dovoljno za validaciju?\n","\n","x_test, y_test = covtype_dataset.data, covtype_dataset.target\n","x_test, y_test = np.array(x_test), np.array(y_test)\n","x_test = (x_test - np.mean(x_test)) / np.std(x_test)\n","y_test = np.eye(out_features)[y_test - 1]"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":998},"executionInfo":{"elapsed":196425,"status":"error","timestamp":1686657474591,"user":{"displayName":"Filip Topic","userId":"18043200965777659690"},"user_tz":-120},"id":"fYc7QY8uNucR","outputId":"d3da2998-fdef-4b11-bed9-7936cf7e5eb6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Parameters:  {'batch_size': 12, 'hidden_units': (100, 50), 'learning_rate': 0.35}\n","Precision before training: 0.00027\n","Recall before training: 0.01634\n","[Training] epoch=0 misclassified=33.20%\n","[Validation] epoch=0 loss=0.69\n","[Training] epoch=1 misclassified=29.31%\n","[Validation] epoch=1 loss=0.68\n","[Training] epoch=2 misclassified=27.98%\n","[Validation] epoch=2 loss=0.61\n","[Training] epoch=3 misclassified=27.01%\n","[Validation] epoch=3 loss=0.62\n","[Training] epoch=4 misclassified=26.26%\n","[Validation] epoch=4 loss=0.64\n","[Test] misclassified=28.79%\n","Precision after training: 0.71998\n","Recall after training: 0.71209\n","Parameters:  {'batch_size': 12, 'hidden_units': (100, 50), 'learning_rate': 0.035}\n","Precision before training: 0.22977\n","Recall before training: 0.40962\n","[Training] epoch=0 misclassified=35.26%\n","[Validation] epoch=0 loss=0.74\n","[Training] epoch=1 misclassified=31.25%\n","[Validation] epoch=1 loss=0.75\n","[Training] epoch=2 misclassified=30.29%\n","[Validation] epoch=2 loss=0.83\n","[Training] epoch=3 misclassified=29.73%\n","[Validation] epoch=3 loss=2.52\n","Early stopping at epoch 3\n","[Test] misclassified=77.05%\n","Precision after training: 0.39533\n","Recall after training: 0.22947\n","Parameters:  {'batch_size': 12, 'hidden_units': (200, 100), 'learning_rate': 0.35}\n","Precision before training: 0.23775\n","Recall before training: 0.48760\n","[Training] epoch=0 misclassified=33.38%\n","[Validation] epoch=0 loss=0.78\n","[Training] epoch=1 misclassified=29.45%\n","[Validation] epoch=1 loss=0.83\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-9f964f5d34be>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# SGD optimajzer primjenjuje gradijente da se azuriraju parametri modela u smjeru koji smanjuje gresku\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;31m# broj pogresko klasifikovanih u trenutnom becu se racuna poredjenjem indeksa u maksimalnim vrijednostima predvidjanih i stvarnih vrijednosti i sumirajuci razlike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mtotal_misclassified\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-060d476fb7c9>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mparam\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for parameters in parameter_combinations:\n","    # model mreze\n","    model = NeuralNetwork(\n","        [\n","            Linear(in_features, parameters['hidden_units'][0]),\n","            Sigmoid(),\n","            Linear(parameters['hidden_units'][0], parameters['hidden_units'][1]),\n","            Sigmoid(),\n","            Linear(parameters['hidden_units'][1], out_features),\n","            Softmax()\n","        ])\n","\n","    # funkcija greske, racuna nesaglasnost predvidjenog i stvarnog\n","    loss_fn = CrossEntropyLoss()\n","    # stohasticki gradijentni spust, namjesta parametre da se ta greska smanji\n","    optimizer = SGD(model.parameters(), parameters['learning_rate'])\n","\n","    print(\"Parameters: \", parameters)\n","    eval_prec, eval_rec = evaluate_model(model, x_test, y_test)\n","    print(f\"Precision before training: {eval_prec:.5f}\")\n","    print(f\"Recall before training: {eval_rec:.5f}\")\n","\n","    batch_size = parameters['batch_size'] # pravi probleme u f-stringu ako ne stavim u varijablu\n","\n","    lowest_loss = float('inf')\n","    num_bad_epochs = 0\n","\n","    # treniranje\n","    for epoch in range(epochs):\n","        total_misclassified = 0\n","        # mijesanje da se izbjegne overfittovanje specificnom redoslijedu\n","        permutation = np.random.permutation(x_train.shape[0])\n","        # dobija se nasumicna permutacija indeksa kojom se mijesaju x i y trening skupovi\n","        x_train = x_train[permutation]\n","        y_train = y_train[permutation]\n","        # iterira se kroz taj izmjesani trening skup, od 0 do x_train velicine sa batch_size korakom \n","        for i in range(0, x_train.shape[0], batch_size):\n","            model.zero_grad()\n","            x = x_train[i:i+batch_size]\n","            y = y_train[i:i+batch_size]\n","            # 'forward pass' sa isjecenim x podacima, kao gore za racunanje metrika\n","            y_pred = model.forward(x)\n","            # ali se ovde racuna greska izmedju predvidjenih y_pred i stvarnih y\n","            loss = loss_fn.forward(y_pred, y)\n","            # 'backward pass' za gradijent greske, racuna gradijente u odnosu na predvidjene izlaze\n","            grad = loss_fn.backward()\n","            # propagacija greske unazad\n","            model.backward(grad)\n","            # SGD optimajzer primjenjuje gradijente da se azuriraju parametri modela u smjeru koji smanjuje gresku\n","            optimizer.step()\n","            # broj pogresko klasifikovanih u trenutnom becu se racuna poredjenjem indeksa u maksimalnim vrijednostima predvidjanih i stvarnih vrijednosti i sumirajuci razlike\n","            total_misclassified += np.sum(np.argmax(y_pred, axis=1) != np.argmax(y, axis=1))\n","            # iskomentarisano zbog preglednijeg ispisa\n","            # if i % (batch_size * 1000) == 0:\n","            #     print(f'[Training] epoch={epoch} batch={i//batch_size:4}/{x_train.shape[0]//batch_size} loss={loss:.2f}')\n","\n","        percent_misclassified = 100 * total_misclassified / x_train.shape[0]\n","        print(f'[Training] epoch={epoch} misclassified={percent_misclassified:3.2f}%')\n","\n","        validation_loss = loss_fn.forward(model.forward(x_val), y_val)\n","        print(f\"[Validation] epoch={epoch} loss={validation_loss:.2f}\")\n","        if validation_loss < lowest_loss:\n","            lowest_loss = validation_loss\n","            num_bad_epochs = 0\n","        else:\n","            num_bad_epochs += 1\n","            if num_bad_epochs > tolerancy:\n","                print(f\"Early stopping at epoch {epoch}\")\n","                break\n","\n","    total_misclassified = 0\n","    batch_size = 1000\n","    for i in range(0, x_test.shape[0], batch_size):\n","        x = x_test[i:i+batch_size]\n","        y = y_test[i:i+batch_size]\n","        y_pred = model.forward(x)\n","        total_misclassified += np.sum(np.argmax(y_pred, axis=1) != np.argmax(y, axis=1))\n","    percent_misclassified = 100 * total_misclassified / x_test.shape[0]\n","    print(f'[Test] misclassified={percent_misclassified:3.2f}%')\n","\n","    val_prec, val_rec = evaluate_model(model, x_val, y_val)\n","    if val_prec > best_precision:\n","        best_precision = val_prec\n","        best_parameters = parameters\n","\n","    eval_prec, eval_rec = evaluate_model(model, x_test, y_test)\n","    print(f\"Precision after training: {eval_prec:.5f}\")\n","    print(f\"Recall after training: {eval_rec:.5f}\")\n","\n","print(\"Best hyperparameters: \", best_parameters)\n","print(\"Best precision: \", best_precision)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25kNXtPqbfn_"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPTdMVWtAo4R0dyVsb9Wyjl","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
